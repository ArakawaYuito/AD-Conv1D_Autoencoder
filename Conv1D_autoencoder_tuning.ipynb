{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0cd49d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6511909033300535279\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7798259712\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11285291128736656131\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2901478827647995270\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7798259712\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9838609608811257019\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import os\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import precision_score\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import pickle\n",
    "import metric_learn\n",
    "from sklearn.decomposition import PCA  \n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import functools\n",
    "import tempfile\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.python.client import device_lib; print(device_lib.list_local_devices())\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "from utils import make_data, make_data_rev, make_label, calc_leq, leq_filter, validate, figure, figure_detail, plot_timeseries, fig_pr, auc_gs, fig_th_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85fe07",
   "metadata": {},
   "source": [
    "## Conv1D_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b36fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/04 14:54:00 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n",
      "2024/01/04 14:54:00 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2024/01/04 14:54:01 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh()\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial warning can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "    - warn|w|warning|1: for a printed warning\n",
      "    - error|e|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputfilter10_latent_filter10\n",
      "訓練データ：[0, 2, 3, 4, 5]\n",
      "ラベル付与データ：[1]日目\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/04 14:54:04 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: 'Sequential' object has no attribute '_nested_inputs'\n",
      "2024/01/04 14:54:04 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"C:\\Users\\Arakawa\\anaconda3\\envs\\tf291\\lib\\site-packages\\mlflow\\tensorflow\\_autolog.py:52: UserWarning: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "   1/2057 [..............................] - ETA: 2:20:55 - loss: 0.8703WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0044s vs `on_train_batch_end` time: 0.0262s). Check your callbacks.\n",
      " 799/2057 [==========>...................] - ETA: 5s - loss: 0.1021"
     ]
    }
   ],
   "source": [
    "mlflow.autolog(log_models=False)\n",
    "\n",
    "# 実験をアクティブ化(設定)する。ない場合は新たな実験を作成してアクティブ化する\n",
    "mlflow.set_experiment(\"tuning_inputfilter_latent_filter\")\n",
    "run_name='result'\n",
    "best_auc=0\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "#     グリッドサーチ\n",
    "    input_f_s, input_f_e=(10, 14)\n",
    "    latent_f_s, latent_f_e=(10, 14)\n",
    "    for input_f in range(input_f_s, input_f_e):\n",
    "        for latent_f in range(latent_f_s, latent_f_e):\n",
    "            data_dir='filtered_csv_label'\n",
    "            original = pd.read_csv( os.path.join(data_dir, 'learning_data_10.csv') )\n",
    "            df = pd.read_csv( os.path.join(data_dir, 'learning_data_10.csv') )\n",
    "            df=df[df['day']<=9]\n",
    "            test_day=[6, 7, 8, 9]\n",
    "            semi_train_day=[0, 1, 2, 3, 4, 5]\n",
    "\n",
    "            test_df=df[(df['day'].isin(test_day))].reset_index(drop=True)\n",
    "            semi_train_df=df[(df['day'].isin(semi_train_day))].reset_index(drop=True)\n",
    "\n",
    "            df_list=[]\n",
    "            window=16\n",
    "            for i in set(df['day']):\n",
    "                lag=[] \n",
    "                lag.append(make_data_rev(semi_train_df.loc[semi_train_df['day']==i, ['original']], 'original', window))\n",
    "                lag.append(semi_train_df.loc[semi_train_df['day']==i, ['day']])\n",
    "                lag.append(semi_train_df.loc[semi_train_df['day']==i, ['label']])\n",
    "                df_add_lag=pd.concat(lag, axis=1)\n",
    "                df_list.append(df_add_lag)\n",
    "            semi_train_df=pd.concat(df_list)\n",
    "            semi_train_df=semi_train_df.dropna().reset_index(drop=True)\n",
    "            semi_train_df.loc[:, 'convaed']=0\n",
    "            semi_train_df.loc[:, 'convaed_label']=0\n",
    "\n",
    "            # padding=sameは元の入力と同じ長さを出力がもつように入力にパディングする．\n",
    "            # （※正確には，入力の大きさをstridesの大きさで単純に割ったものが出力の大きさになる）\n",
    "            input_filter=input_f\n",
    "            latent_filter=latent_f\n",
    "            kernel_size=6\n",
    "            print(f'inputfilter{input_filter}_latent_filter{latent_filter}')\n",
    "\n",
    "            conv_autoencoder = keras.Sequential(\n",
    "                [\n",
    "                    layers.Conv1D(\n",
    "                        filters=input_filter,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=\"same\",\n",
    "                        strides=2,\n",
    "                        activation=\"relu\",\n",
    "                    ),\n",
    "\n",
    "                    layers.Conv1D(\n",
    "                        filters=latent_filter,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=\"same\",\n",
    "                        strides=2,\n",
    "                        activation=\"relu\",\n",
    "                    ),\n",
    "                    layers.Conv1DTranspose(\n",
    "                        filters=latent_filter,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=\"same\",\n",
    "                        strides=2,\n",
    "                        activation=\"relu\",\n",
    "                    ),\n",
    "\n",
    "                    layers.Conv1DTranspose(\n",
    "                        filters=input_filter,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=\"same\",\n",
    "                        strides=2,\n",
    "                        activation=\"relu\",\n",
    "                    ),\n",
    "                    layers.Conv1DTranspose(filters=1, kernel_size=kernel_size, padding=\"same\"),\n",
    "                ]\n",
    "            )\n",
    "            # テストデータとする日\n",
    "            i=[1]\n",
    "            label_train_day=[k for k in semi_train_day if k not in i]\n",
    "            label_grant_day=i\n",
    "            print(f'訓練データ：{label_train_day}')\n",
    "            print(f'ラベル付与データ：{label_grant_day}日目')\n",
    "            semi_train_x=semi_train_df[(semi_train_df['day'].isin(label_train_day))][semi_train_df.columns[:-4]].reset_index(drop=True)\n",
    "            labeling_x=semi_train_df[(semi_train_df['day'].isin(label_grant_day))][semi_train_df.columns[:-4]].reset_index(drop=True)\n",
    "            train_mean=semi_train_x['original'].mean()\n",
    "            train_std=semi_train_x['original'].std()\n",
    "            \n",
    "             # Log the hyperparameters\n",
    "            mlflow.log_params({'trainday':label_train_day, 'testday':label_grant_day, \n",
    "                               'window_size':window, 'input_filter_range':f'range({input_f_s},{input_f_e})', \n",
    "                              'latent_filter_range':f'range({latent_f_s},{latent_f_e})', 'kernel_size':kernel_size})\n",
    "\n",
    "            #標準化\n",
    "            train_data = (semi_train_x.values-train_mean)/train_std\n",
    "            labeling_data = (labeling_x.values -train_mean) / train_std\n",
    "\n",
    "            # データをモデルに入力する形状に整形し,データの型をtf.float32に変換\n",
    "            train_data = tf.cast(train_data[:, :, np.newaxis], tf.float32)\n",
    "            labeling_data = tf.cast(labeling_data[:, :, np.newaxis], tf.float32)\n",
    "\n",
    "            conv_autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                                     loss='mae')\n",
    "\n",
    "            callback = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0,\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "            history = conv_autoencoder.fit(train_data, train_data, \n",
    "                  epochs=2, \n",
    "                  batch_size=1024,\n",
    "                  validation_data=(labeling_data, labeling_data),\n",
    "                  callbacks=[callback, tensorboard_callback],\n",
    "                  shuffle=True)\n",
    "\n",
    "            plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "            plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            reconstructions = conv_autoencoder.predict(labeling_data)\n",
    "            #     異常度算出\n",
    "            d= tf.keras.losses.mae(reconstructions[:, :, 0], labeling_data[:, :, 0]).numpy()\n",
    "            semi_train_df.loc[semi_train_df['day'].isin(label_grant_day), 'convaed']=d\n",
    "            print('------------------------------------------------------')\n",
    "\n",
    "            test_v=semi_train_df.loc[semi_train_df['day'].isin(label_grant_day), ['original', 'label']].copy().reset_index(drop=True)\n",
    "            d_test=(d-d.min())/(d.max()-d.min())\n",
    "\n",
    "            thr_bins=10\n",
    "            p_score, r_score, f_score, thresholds, thr, auc = fig_pr(test_v, d_test, thr_bins)\n",
    "            f_max=f_score.max()\n",
    "            p_max=p_score[np.argmax(f_score)]\n",
    "            r_max=r_score[np.argmax(f_score)]\n",
    "            print('\\033[31m'+f'AUC：{auc}'+'\\033[0m')\n",
    "            print(f'理想の閾値：{thr}')\n",
    "            print(f'F値（最高）：{f_max}')\n",
    "            print(f'適合率（F値が最高の時）：{p_max}')\n",
    "            print(f'再現率（F値が最高の時）：{r_max}')\n",
    "\n",
    "            if best_auc<auc:\n",
    "                best_auc=auc\n",
    "                best_parameters = {'input_filter': input_filter, 'latent_filter':latent_filter}\n",
    "                # ベストパラメータを上書き\n",
    "                mlflow.log_metric(\"best_parameter-input_filter\", best_parameters['input_filter'])\n",
    "                mlflow.log_metric(\"best_parameter-latent_filter\", best_parameters['latent_filter'])\n",
    "\n",
    "                ################################################\n",
    "                # Log the loss metric\n",
    "                mlflow.log_metric(\"best_test_AUC\", auc)\n",
    "                mlflow.log_metric(\"best_test_Fscore\", f_max)\n",
    "                mlflow.log_metric(\"best_test_Precision_when Fscore Best\", p_max)\n",
    "                mlflow.log_metric(\"best_test_Recall_when Fscore Best\", r_max)\n",
    "\n",
    "                # 一時ディレクトリに予測結果のグラフを保存して，mlflowに送信する\n",
    "                with tempfile.TemporaryDirectory()  as tmp:\n",
    "                    filename = os.path.join(tmp, \"predict_results.png\")\n",
    "                    plot_timeseries([test_v['original'].values], test_v['label'].values, d=d_test, thr=thr, mlflow=filename)\n",
    "                    mlflow.log_artifact(filename, artifact_path=\"plot_timeseries\")\n",
    "\n",
    "            #     モデルの構造を定義しているソースコードを保存\n",
    "                mlflow.log_artifact('Conv1D_autoencoder_tuning.ipynb', artifact_path=\"source_code\")    \n",
    "                # Log the model\n",
    "                signature= infer_signature(labeling_data.numpy()[:2], conv_autoencoder.predict(labeling_data.numpy()[:2]))\n",
    "                input_example= labeling_data.numpy()[:2]\n",
    "                mlflow.tensorflow.log_model(\n",
    "                    conv_autoencoder, \n",
    "                    artifact_path=\"Conv1D_autoencoder\",\n",
    "                    signature=signature,\n",
    "                    registered_model_name=\"Conv1D_autoencoder\",\n",
    "                    input_example= input_example\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a03e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mlflow gc --backend-store-uri ./mlruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb8a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mlflow ui --port 5003 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf291",
   "language": "python",
   "name": "tf291"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
